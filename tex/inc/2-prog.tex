\section{Параллельный алгоритм и программная реализация}

В данной главе представлено описание разработанного параллельного алгоритма и основных компонентов программного обеспечения, 
с помощью которого произведено исследование вычислительных особенностей спектрально-разностного алгоритма в неоднородных средах.
Представлена эффективная последовательная и параллельная реализация алгоритма под системы с общей памятью,
на языке программирования С++ с использованием стандарта С++17, в том числе под вычислительные системы использующиеся в Сибирском Суперкомпьютерном 
центре Коллективного Пользования ИВМиМГ СО РАН (ЦКП ССКЦ ИВМиМГ СО РАН). В ходе работы проведена  адаптация параллельной реализации алгоритма под архитектуры, поддерживающие векторные расширения (SIMD).
Также рассмотренна одна из возможных параллельных реализаций для гетерогенных систем с распределенной памятью
при помощи системы фрагментированного программирования LuNA, разрабатываемой лабораторией синтеза параллельных программ ИВМиМГ СО РАН.
Проведено сравнение между программами: сгенерированной системой LuNA для вычислительных систем с общей памятью и реализованной средствами OpenMP.

\subsection{Задание основных параметров модели}

Для того, чтобы провести численный расчет приведенной задачи в первую очередь необходимо задать сеточную область.
В настоящее время существует множество программных средств, позволяющих это реализовать,
однако большинство из них входит в состав больших коммерческих программных продуктов, 
использование которых избыточно для данной задачи.

В первую очередь задаются основные параметры модели: $a$, $b$ - размеры исследуемоей области.
Количество шагов по времени $t$ и размеры сеточной области по вертикальной координате $z$, 
а также количество гармоник, используемых для преобразования Фурье. Для того, чтобы обеспечить
сходимость явной разностной схемы, необходимо проверить выполнение условий Куранта.

Для того, чтобы не тратить вычислительные ресурсы во время исполнения основного кода и автоматически проверять корректность заданных условий перед постановкой задачи в очередь на кластере, 
удобно вычислять все параметры на этапе компиляции программы, условия Куранта и остальные ограничения на входные данные
могут быть проверены аналогичным образом.

Средства С++17 позволяют реализовать эти проверки на этапе компиляции. Например, следующим образом.

\begin{lstlisting}[style={CppCodeStyle}]
using Precision = double;

constexpr auto g_z_limit_value = static_cast<Precision>(1.);
constexpr auto g_t_limit_value = static_cast<Precision>(1.);

constexpr auto g_t_grid_size = 2048u;
constexpr auto g_z_grid_size = 1024u;

constexpr auto g_k_limit = 32u;

constexpr auto g_z_grid_step = g_z_limit_value / static_cast<Precision>(g_z_grid_size);
constexpr auto g_t_grid_step = g_t_limit_value / static_cast<Precision>(g_t_grid_size);

static_assert(g_t_grid_step * g_t_grid_step <= g_z_grid_step * g_z_grid_step / static_cast<Precision>(2.));

static_assert(g_z_grid_step < static_cast<Precision>(1.) / (static_cast<Precision>(2.) * static_cast<Precision>(g_k_limit)));
\end{lstlisting}

Параметры среды $\overline\rho$, $\overline\lambda$, $\overline\mu$ задаются в виде 2D массивов, 
где первая координата соответствует индексу по пространственной сетке $z$, а вторая соответствует индексу гармоники $k$.
Аналогичным образом задаются рассчитываемые величины $\overline u$, $\overline w$, $\overline p$, $\overline q$, $\overline s$.

После формирования массивов, заполненных коэффициентами среды, при помощи БПФ происходит переход от $\overline\rho$, $\overline\lambda$, $\overline\mu$ к их частотному представлению. На этом этапе также возможно проведение дополнительной предобработки этих данных (например, фильтрации) с целью улучшения дискретного представления спектра параметров среды. Все эти преобразования производятся один раз и поэтому оказывают минимальное влияние на время работы программы. 

\subsection{Последовательный алгоритм}

Алгоритм расчета явной разностной схемы (\ref{eq:9}), представляет собой процедуру вычисления серии сумм типа сверток и корреляций,
представленных в (\ref{eq:6}). Вычисление каждой из таких сумм, предварительно сопровождается умножением элементов исходных 
массивов на соотвествующие коэффициенты, после чего выполняется дискретная свертка или корреляция.

Так как полная реализация алгоритма средствами С++17 является достаточно громоздкой, ниже приведена только блоксхема (рис. \ref{scheme}) описываемоего алгоритма.

\addimghere{scheme}{0.55}{Схема последовательного исполнения}{scheme}

Стоит отметить, что описываемый метод решения исходной задачи относится к так называемым memory-bounded алгоритмам.
Поэтому для более эффективного доступа к памяти, и как следствие, более эффективному исполнению,
становится важным правильно расположить циклы по времени, пространственной координате и гармоникам.

\begin{lstlisting}[style={CppCodeStyle}]
for (t=0; t < t_limit; ++t)
{
	for (z=0; z < z_limit; ++z)
	{
		§//! Умножаем на соотвествующие k-коэффициенты§
		calculate_coefs_corr_and_conv(values_1[z], t);
		§//! Вычисляем свертки и корреляции§
		calculate_all_corr_and_conv(values_1[z], t);
		for (k=0; k < k_limit; ++k)
		{
			§//! Применяем схему для соотвествующих z, k и функции источника§
			values_2[z][k] = apply_scheme(values_1[z][k], t, f);
		}
	}
	§//! Сохраняем результат вычисления шага t+1§
	save(values_2);
	§//! Значения шага t+1 задаем как исходные для следующего шага§
	swap(values_1, values_2);
}
\end{lstlisting}

Для реальных численных задач число шагов по $z$ координате обычно значительно больше, чем число гармоник используемых для рассчетов,
так же вычисление сумм типа сверток требует больше машинного времени, чем вычисление самих коэфициентов разностной схемы.
Таким образом "бутылочным горлышком" такого алгоритма является скорость вычисления сумм типа сверток и корреляций для
каждого из узлов по $z$ координате.

Реализация эффективных операций сверток и корреляций может потребовать от программиста глубоких знаний архитектуры 
целевой вычислительной платформы, поэтому для вычислений было принятно решение использовать,
хорошо зарекомендовавшую себя библиотеку Intel MKL, которая предоставляет процедуры для эффективного вычисления сверток/корреляций.

\subsection{Параллельный алгоритм}
Для проведения численных экспериментов с полномасштабной моделью (с большим количеством узлов расчетной сетки и шагов по времени)
требуется значительное число вычислительных ресурсов. Поэтому, чтобы получить приемлимое время расчетов для эксперимента,
потребовалось провести распараллеливание исходной последовательной программы.

В настоящее время, существует множество различны многоядерных вычислительных комплексов, в числе которых, как комплексы с общей памятью,
так и вычислительные кластера, в самом общем случае, с гетерогенными узлами.

В данной работе вычисления и изучение результатов распаралеливания проводились, как на персональном компьютере с процессором Intel Core i9 2,4 ГГц и
32 Гб DDR4 памяти, так и на отдельных серверах с общей памятью ЦКП ССКЦ ИВМиМГ СО РАН.

Также стоит отметить, что одним из основных требований к исходной программе была ее переносимость между различными платформами,
что позволило провести большое количество численных экпериметов на различных вычислительных системах,
без необходимости в существенной доработке исходной программы.

\subsubsection{Параллельная реализация с использованием OpenMP}

Для распараллеливания исходной схемы итерации в цикле по координате $z$ распределяются между параллельными потоками средствами распараллеливания и блокирующей синхронизации OpenMP (рис. \ref{par-scheme}).

Заметим, что такой вариант распараллеливания не является наиболее эффективным из-за того, что не дает программисту полного контроля
над исполнением паралленьной программы, в частности над операциями синхронизации, которые при данном "наивном" варианте распараллеливания
занимают значительную часть времени расчета. Однако такой подход позволяет получить значительное ускорение исполнения программы,
при минимальной модификации исходного пользовательского кода.

\addimghere{par-scheme}{1.}{Схема параллельного исполнения c использованием OpenMP}{par-scheme}

\subsubsection{Параллельная реализация с использованием системы LuNA}

Необходимым требованием к параллельной программе является эффективность ее работы. Эффективность понимается в смысле времени выполнения,
расхода памяти, нагрузки на коммуникационную подсистему и т.п.
Поэтому написание оптимальной программы для гетерогенных систем, реализующей представленный численный алгоритм,
удовлетворяющей требованием описанным выше, может представлять значительные трудности.
Так как исследователю приходится решать задачи из области системного программирования,
которые не относятся к хорошо знакомой ему прикладной предметной области.
В связи с этим, актуальным становится автоматизация конструирования параллельной программы.
Одним из возможных решений, которое позволяет избежать описанных выше трудностей, может быть система фрагментированного программирования LuNA,
разработанная лабораторией Синтеза параллельных программ под руководством В.Э.Малышкина.

Описание исходной программы для системы LuNA происходит в несколько этапов:

\begin{itemize}
    \item Описание наборов входных/выходных фрагментов данных(ФД) и вычислений(ФВ), связанных информационными зависимостями
на любом из доступных языков программирования(С, С++, Fortran);
    \item Описание фрагментированной программы на функциональном языке LuNA (приложение А);
    \item Исполнение скомпилированного кода на системе LuNA RTS;
\end{itemize}

Алгоритм исполнения программы (рис. \ref{luna}) состоит из следующих шагов, которые могут повторяться:
\begin{itemize}
    \item Фрагмент вычислений исполняется, при условии, что все его выходные фрагменты данных готовы.
    \item После завершения вычислений выходные фрагменты получают свои значения.
    \item После того, как были вычислены все фрагменты данных, программа завершается.
\end{itemize}

\addimghere{luna}{0.85}{Схема исполнения фрагментированной программы}{luna}

Фрагменты вычислений являются аналогами функций процедурных языков программирования,
а фрагменты данных – переменных единственного присваивания.
Поток управления для программ на LuNA не задан, фрагмент вычислений исполняется только если все его входные фрагменты данных получили значения \cite{mach}.

Средства С++ позволяют реализовать описание пользовательских фрагментов-вычислений в виде динамической библиотеки,
для дальнейшего динамического связывания(dynamic linking) с параллельным программным кодом, сгенерированным компилятором системы LuNA,
согласно соглашению о вызовах языка C.

Преимуществом такого подхода к описанию параллельной программы является его простота. Программисту
не требуется решать задачи прикладного системного программирования, которые могут возникнуть, например, при использовании средств MPI.
Задачи балансировки нагрузки, обмена границами и т.п. возьмет на себя система.

Полагая, что размер $N$ сетки по $z$ соотносится с размерами фрагментов $M$ и их количеством $K$,
следующим образом $N = M * K$, вычислительный процесс может быть реализован аналогичным с (рис. \ref{par-scheme}) образом.
Главным отличием будет то, что в самом общем случае, каждый фрагмент может быть исполненен на любом из доступных узлов,
в произвольном порядке, не нарущающем информационных зависимостей.

Отдельно отметим практическую значимость такого подхода.
Повышение уровня программирования, что является целью разработки LuNA \cite{mach},
достигается за счет следующих задач, решаемых системой исполнения:
\begin{itemize}
    \item Асинхронный запуск фрагментов-вычислений.
    \item Выбор запускаемых фрагментов-вычислений из списка готовых фрагментов-вычислений.
    \item Локальная балансировка нагрузки.
    \item Порождение, распределение по распределенной памяти фрагментов данных.
Сборка мусора.
	\item Обработка исключительных ситуаций. Отказоустойчивость.
\end{itemize}

\subsection{Адаптация и оптимизация под архитектуры}
Не смотря на то, что параллельная реализация с использованием средств OpenMP и Intel MKL дает существенный выйгрыш,
по сравнению с последовательной реализацией, чтобы добиться пиковой производительности нужно учитывать особенности
архитектуры конкретной вычислительной системы, и применять точечные оптимазации отдельных программных процедур.

\subsubsection{Оптимизации с использованием векторных расширений}

Одними из популярных способов такого рода оптимизаций является использование специальных инструкций процессора,
так называемых векторных расширений (SIMD), которые позволяют ускорить исполнение процедур, например, посредством
разворачивания циклов или использования специальных векторных регистров и операций над ними.

Библиотека OpenMP позволяет векторизовать пользовательский код с помощью специальных директив препроцессора C++, а средства
современных компиляторов позволяют получать отчеты о векторизации на этапе комплиляции.
Например, флагами компиляции -Rpass="loop|vect" -Rpass-missed="loop|vect" -Rpass-analysis="loop|vect" для компилятора LLVM Clang.

Пример векторизованного средствами OpenMP кода на C++, для функции вычисления коэффициентов для последющего вычисления сумм типа сверток.

\begin{lstlisting}[style={CppCodeStyle}]
inline auto apply_corr_factor(
	const Grid1D& input,
	const Precision mult,
	Grid1D& result) noexcept
{
  #pragma omp simd
  for (auto k_idx = 0u; k_idx < input.size(); ++k_idx)
  {
    result[k_idx] = mult 
    	* static_cast<Precision>(input.size() - k_idx)
    	* input[k_idx];
  }
}
\end{lstlisting}

При таком подходе набор используемых векторных инструкций, параметры разворачивания циклов и другие, будут подобраны библиотекой автоматически,
что может быть не эффективно для некоторых программных процедур. Поэтому в данной работе исследовался вариант с ручной векторизацией,
что позволило незначительно, но все же ускорить исполнение пользовательских функций.

Вариант с ручной векторизацией с использованием векторных расширений AVX2.

\begin{lstlisting}[style={CppCodeStyle}]
inline auto apply_corr_factor(
	const Grid1D& input,
	const Precision mult,
	Grid1D& result) noexcept
{
  static_assert(sizeof(Grid1D::value_type) == 8u);
  for (auto idx = 0u; idx < input.size() / 4u; ++idx)
  {
    inline const auto k_idx = idx * 4u;
    inline auto input_data = _mm256_load_pd(input.data() + k_idx);
    inline const auto factors_data = _mm256_set_pd(
        mult * static_cast<Precision>(input.size() - k_idx - 3u),
        mult * static_cast<Precision>(input.size() - k_idx - 2u),
        mult * static_cast<Precision>(input.size() - k_idx - 1u),
        mult * static_cast<Precision>(input.size() - k_idx));
    inline const auto factorized = _mm256_mul_pd(input_data, factors_data);
    _mm256_store_pd(result.data() + k_idx, factorized);
  }
  for (
  	auto k_idx = input.size() - input.size() \% 4u;
  	k_idx < input.size();
  	++k_idx) 
  {
    result[k_idx] = mult
    	* static_cast<Precision>(input.size() - k_idx)
    	* input[k_idx];
  }
}
\end{lstlisting}

Для сравнения результатов различных векторизаций использовалась библиотека Google Benchmark.
Сравнения выполнены на процессоре Intel Core-i9(16 X 2300 MHz CPUs) L3 Unified 16384 KiB, на последних доступных
на момент написания работы версиях компиляторов.

\addimghere{intel-vect}{0.85}{Результаты векторизации для Intel C++ Intel Core-i9(16 X 2300 MHz CPUs) L3 Unified 16384 KiB}{intel-vect}
\addimghere{gcc-vect}{0.85}{Результаты векторизации для GNU GCC Intel Core-i9(16 X 2300 MHz CPUs L3 Unified 16384 KiB)}{gcc-vect}
\addimghere{apple-vect}{0.85}{Результаты векторизации для LLVM Clang Intel Core-i9(16 X 2300 MHz CPUs L3 Unified 16384 KiB)}{apple-vect}

На графиках для компиляторов GNU GCC и LLVM Clang (рис. \ref{gcc-vect}, рис. \ref{apple-vect}) аномальных результатов не наблюдается,
тогда как, для компилятор Intel C++ векторизация средствами OpenMP оказывается медленнее, чем векторизация выполненная
этим компилятором автоматически (рис. \ref{intel-vect}).

\subsubsection{Сравнение автовекторизации различных компиляторов}

Современные компиляторы, при правильно заданных опциях компиляции, достаточно хорошо векторизуют код,
при этом от программисту не требуется никаких дополнительных вмешательств от разработчика ПО.
Поэтому еще одной темой представляющей интерес является сравнение производительности кода,
скомпилированного различными компиляторами (рис. \ref{intel-gcc-clang}).

\addimghere{intel-gcc-clang}{0.85}{Сравнение автовекторизации Intel Core-i9(16 X 2300 MHz CPUs) L3 Unified 16384 KiB}{intel-gcc-clang}

Из приведенного графика (рис. \ref{intel-gcc-clang}) видно, что наилучший результат автоматической векторизации показывают
компиляторы Intel C++ и LLVM Clang, синяя и зеленая линии соответственно.

\subsubsection{Выравнивание памяти}

Еще одним широкоприменяемым способом ускорения исполнения векторизованного кода является выравнивание виртуальных адресов
для динамически аллоцируемых объектов. Этот способ позволяет организовать более эффективный доступ к памяти вычислительной системы,
по средствам более эффективной утилизации кэшей L3, L2, L1.

Стандартный аллокатор памяти C++(std::allocator) выравнивает объекты, аллоцируемые динамически, на размер их типа,
огрубляя полученное значение до ближайшей степени двойки.
Однако полученное таким образом значение выравнивания не может превосходить платформенно-зависимого значения sizeof(std::max\_align\_t),
обычно равное 16 байтам.

Для большинства AVX/SSE инструкций отсутствуют требования к выравниванию операндов в памяти. 
Однако разработчиками этих расширений рекомендуется следить за выравниванием на размер операнда
во избежание значительного снижения производительности \cite{intel}.

Поэтому в рамках данной работы был реализован клаас aligned\_allocator - аллокатор выравненный
на 16 и 32 байт для SSE и AVX/AVX2 расширений соотвественно.
Класс был реализован в соотвествии с требованиями на интерфейс аллокатора C++17, поэтому может быть использован совместно с любым
контейнером из стандартной библиотеки, например, с контейнером std::vector.

\clearpage
