\section{Программная реализация}

В данной главе представленно описание основых компонентов разработанного параллельного программного обеспечения, 
с помощью которого было произведено исследование спектрально-разностного алгоритма в однородных средах.
Будет описана эффективная последовательная и параллельная реализация алгоритма под системы с общей памятью,
на языке программирования С++ с использованием стандарта С++17, в том числе под вычислительные системы использующиеся в Сибирском Суперкомпьютерном 
центре Коллективного Пользования ИВМиМГ СО РАН (ЦКП ССКЦ ИВМиМГ СО РАН). Описана тема адаптации
и ускорения исполнения параллельной реализации алгоритма под архитектуры, поддерживающие векторные расширения (SIMD).
Также будет рассмотренна одна из возможных параллельных реализаций под системы с распределенной памятью при помощи системы кодогенерации LuNA,
разрабатываемой лабораторией синтеза параллельных программ ИВМиМГ СО РАН.

\subsection{Задание основных параметров модели}

Для того, чтобы провести численный расчет приведенной задачи в первую очередь необходимо задать сеточную область.
В настоящее время существует множество программных средств, позволяющих это реализовать,
однако большинство из них входит в состав больших коммерческих программных продуктов, 
использование которых может быть избыточным для данной задачи.

В первую очередь задаются основные параметры модели: $a$, $b$ - размеры исследуемоей области.
Размеры сеточной области по времени $t$ и вертикальной координате $z$, 
а также количество используемых для преобразования Фурье гармоник. Также для того, чтобы обеспечить
сходимость явной разностной схемы по $z$ координате, необходимо проверить выполнение условий Куранта.

Для того, чтобы не тратить вычислительных ресурсов во время исполнения основного кода, 
удобно вычислять все параметры на этапе компиляции программы, условия Куранта и остальные ограничения на входные данные
могут быть проверены аналогичным образом.

Средствами С++17 эти проверки можно реализовать, например, следующим образом.

\begin{lstlisting}[style={CppCodeStyle}]
using Precision = double;

inline static constexpr auto g_z_limit_value = static_cast<Precision>(1.);
inline static constexpr auto g_t_limit_value = static_cast<Precision>(1.);

inline static constexpr auto g_t_grid_size = 2048u;
inline static constexpr auto g_z_grid_size = 1024u;

inline static constexpr auto g_k_limit = 32u;

inline static constexpr auto g_z_grid_step = g_z_limit_value / static_cast<Precision>(g_z_grid_size);
inline static constexpr auto g_t_grid_step = g_t_limit_value / static_cast<Precision>(g_t_grid_size);

static_assert(g_t_grid_step * g_t_grid_step <= g_z_grid_step * g_z_grid_step / static_cast<Precision>(2.));

static_assert(g_z_grid_step < static_cast<Precision>(1.) / (static_cast<Precision>(2.) * static_cast<Precision>(g_k_limit)));
\end{lstlisting}

Параметры среды $\overline\rho$, $\overline\lambda$, $\overline\mu$ задаются в виде 2D массивов, 
где первая координата соответсвует индексу по переменной $z$, а вторая соответствует индексу гармоники $k$
Аналогичным образом задаются расчитываемые величины $\overline u$, $\overline w$, $\overline p$, $\overline q$, $\overline s$.

\subsection{Последовательный алгоритм}

Алгоритм расчета явной разностной схемы (todo refence), представляет собой процедуру вычисления серии, сумм типа сверток и корреляций,
представленных в (\ref{eq:6}). Вычисление каждой из таких сумм, предварительно сопровождается умножением элементов исходных 
массивов на соотвествующие коэффициенты, после чего выполняется дискретная свертка или корреляция.

Так как полная реализация алгоритма средствами С++17 является достаточно громоздкой, ниже приведен только псевдокод описываемого алгоритма.
Полная реализация алгоритма может быть найдена в приложении (todo reference).

\begin{lstlisting}[style={CppCodeStyle}]

§//! Параметры среды§
struct EnvParams
{
	rho[z_limit][k_limit];
	lamda[z_limit][k_limit];
	mu[z_limit][k_limit];
};

§//! Искомые величины§
struct ModelParams
{
	u[z_limit][k_limit];
	w[z_limit][k_limit];
	... 
}

§//! Предрасчитанная функция истоничка§
f[t_limit] = calculate_f();

EnvParams env = set_env(); §//! Задаем параметры среды§

ModelParams values_1 = set_zero(); §//! Задаем начальные значения равные 0§
ModelParams values_2 = set_zero(); §//! Задаем начальные значения равные 0§

for (t=0; t < t_limit; ++t)
{
	for (z=0; z < z_limit; ++z)
	{
		§//! Умножаем на соотвествующие k-коэффициенты§
		calculate_coefs_corr_and_conv(values_1[z], t);
		§//! Вычисляем свертки и корреляции§
		calculate_all_corr_and_conv(values_1[z], t);
		for (k=0; k < k_limit; ++k)
		{
			§//! Применяем схему для соотвествующих z, k и функции источника§
			values_2[z][k] = apply_scheme(values_1[z][k], t, f);
		}
	}
	§//! Сохраняем результат вычисления шага t+1§
	save(values_2);
	§//! Значения шага t+1 задаем как исходные для следующего шага§
	swap(values_1, values_2);
}

\end{lstlisting}

Для реальных численных задач число шагов по $z$ координате обычно значительно больше, чем число гармоник используемых для рассчетов,
так же вычисление сумм типа сверток требует больше машинного времени, чем вычисление самих коэфициентов разностной схемы.
Таким образом "бутылочным горлышком" спектрально-разностного метода является скорость вычисления сумм типа сверток и корреляций для
каждого из узлов по $z$ координате.

Реализация эффективных операций сверток и корреляций может потребовать от программиста глубоких знаний архитектуры 
целевой вычислительной платформы, поэтому для вычислений было принятно решение использовать,
хорошо зарекомендовавшую себя библиотеку Intel MKL, которая предоставляет процедуры для эффективного вычисления сверток/корреляций.

\subsection{Параллельный алгоритм}
Проведение численных экспериментов с полномасштабной моделью(с большим количеством узлов расчетной сетки)
требуется значительное число вычислительных ресурсов. Поэтому, чтобы получить приемлимое время расчетов для эксперимента,
потребовалось провести распараллеливание исходной последовательной программы.

В настоящее время, существует множество различны многоядерных вычислительных комплексов, в числе которых, как комплексы с общей памятью,
так и вычислительные кластера, в самом общем случае, с гетерогенными узлами.

В данной работе вычисления и изучение результатов распаралеливания проводились, как на персональном компьютере с процессором Intel Core i9 2,4 ГГц и
32 Гб DDR4 памяти, так и на серверах с общей памятью ЦКП ССКЦ ИВМиМГ СО РАН с конфигурацией (todo).

Также стоит отметить, что одним из основных требований к исходной программе была ее переносимость между различными платформами,
что позволило провести большое количество численных экпериметов, без необходимости в существенной доработке исходной программы.

\subsubsection{Параллельная реализация с использованием OpenMP}

Для распараллеливания исходной схемы исходная расчетная область по $z$ координате разбивается на "отрезки",
расчет в каждом из отрезков проводится средстами распараллеливания и блокирующей синхронизации OpenMP.

todo графики сравнения параллельной и последовательной реализации

todo графики количества потоков параллельной реализации

Заметим, что такой вариант распараллеливания не является наиболее эффективным из-за того, что не дает программисту полного контроля
над исполнением паралленьной программы, в частности над операциями синхронизации, которые при данном "наивном" варианте распараллеливания
занимают значительную часть времени расчета. Однако такой подход позволяет получить значительное ускорение исполнения программы,
при минимальной модификации исходного пользовательского кода.

Наиболее эффективный результат дает комбинация Intel MKL и OpenMP.

todo графики

\subsubsection{Параллельная реализация с использованием системы LuNA}

(todo)

\subsection{Адаптация и оптимизация под архитектуры}
Не смотря на то, что параллельная реализация с использованием средств OpenMP и Intel MKL дает существенный выйгрыш,
по сравнению с последовательной реализацией, чтобы добиться пиковой производительности нужно учитывать особенности
архитектуры конкретной вычислительной системы, и применять точечные оптимазации отдельных программных процедур.

\subsubsection{Оптимизации с использованием векторных расширений}

Одними из популярных способов такого рода оптимизаций является использование специальных инструкций процессора,
так называемых векторных расширений (SIMD), которые позволяют ускорить исполнение процедур, например, по средствам
разворачивания циклов или использования специальных векторных регистров и операций над ними.

Библиотека OpenMP позволяет векторизовать пользовательский код с помощью специальных директив препроцессора C++, а средства
современных компиляторов позволяют получать отчеты о векторизации на этапе комплиляции.
Например, флагами компиляции -Rpass="loop|vect" -Rpass-missed="loop|vect" -Rpass-analysis="loop|vect" для компилятора LLVM Clang.

Пример векторизованного средствами OpenMP кода на C++, для функции вычисления коэффициентов для последющего вычисления сумм типа сверток.

\begin{lstlisting}[style={CppCodeStyle}]
inline auto apply_corr_factor(
	const Grid1D& input,
	const Precision mult,
	Grid1D& result) noexcept
{
  #pragma omp simd
  for (auto k_idx = 0u; k_idx < input.size(); ++k_idx)
  {
    result[k_idx] = mult 
    	* static_cast<Precision>(input.size() - k_idx)
    	* input[k_idx];
  }
}
\end{lstlisting}

При таком подходе набор используемых векторных инструкций, параметры разворачивания циклов и другие, будут подобраны библиотекой автоматически,
что может быть не эффективно для некоторых программных процедур. Поэтому в данной работе исследовался вариант с ручной векторизацией,
что позволило незначительно, но все же ускорить исполнение пользовательских функций.

Вариант с ручной векторизацией с использованием векторных расширений AVX2.

\begin{lstlisting}[style={CppCodeStyle}]
inline auto apply_corr_factor(
	const Grid1D& input,
	const Precision mult,
	Grid1D& result) noexcept
{
  static_assert(sizeof(Grid1D::value_type) == 8u);
  for (auto idx = 0u; idx < input.size() / 4u; ++idx)
  {
    inline const auto k_idx = idx * 4u;
    inline auto input_data = _mm256_load_pd(input.data() + k_idx);
    inline const auto factors_data = _mm256_set_pd(
        mult * static_cast<Precision>(input.size() - k_idx - 3u),
        mult * static_cast<Precision>(input.size() - k_idx - 2u),
        mult * static_cast<Precision>(input.size() - k_idx - 1u),
        mult * static_cast<Precision>(input.size() - k_idx));
    inline const auto factorized = _mm256_mul_pd(input_data, factors_data);
    _mm256_store_pd(result.data() + k_idx, factorized);
  }
  for (
  	auto k_idx = input.size() - input.size() \% 4u;
  	k_idx < input.size();
  	++k_idx) 
  {
    result[k_idx] = mult
    	* static_cast<Precision>(input.size() - k_idx)
    	* input[k_idx];
  }
}
\end{lstlisting}

Для сравнения результатов различных векторизаций использовалась библиотека Google Benchmark.

(todo оформленные результаты бенчмарков)

\subsubsection{Сравнение автовекторизации различных компиляторов}

Современные компиляторы, при правильно заданных опциях компиляции, достаточно хорошо векторизуют код,
при этом от программиста не требуется никаких дополнительных вмешательств от разработчика ПО.
Поэтому еще одной темой представляющей интерес является сравнение производительности кода,
скомпилированного различными компиляторами.

(todo intel vs gcc vs clang)

\subsubsection{Выравнивание памяти}

Еще одним широкоприменяемым способом ускорения исполнения векторизованного кода является выравнивание виртуальных адресов
для динамически аллоцируемых объектов. Этот способ позволяет организовать более эффективный доступ к памяти вычислительной системы,
по средствам более эффективной утилизации кэшей L3, L2, L1 уровней.

Стандартный аллокатор памяти C++(std::allocator) выравнивает объекты, аллоцируемые динамически, на размер их типа,
огрубляя полученное значение до ближайшей степени двойки.
Однако полученное таким образом значение выравнивания не может превосходить платформенно-зависимого значения sizeof(std::max\_align\_t),
обычно равное 16 байтам.

Для большинства AVX/SSE инструкций отсутствуют требования к выравниванию операндов в памяти. 
Однако разработчиками этих расширений рекомендуется следить за выравниванием на размер операнда
во избежание значительного снижения производительности.

Поэтому в рамках данной работы был реализован клаас aligned\_allocator - аллокатор выравненный
на 16 и 32 байт для AVX и SSE расширений соотвественно.
Класс был реализован в соотвествии с требованиями на интерфейс аллокатора C++17, поэтому может быть использован совместно с любым
контейнером из стандартной библиотеки, например, с контейнером std::vector.

(todo aligned vs default)

\clearpage